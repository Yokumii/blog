
<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>YokumiのBlog</title>
        <link rel="icon" href="/blog/images/favicon.ico" type="image/x-icon">
        <link rel="shortcut icon" href="/blog/images/favicon.ico" type="image/x-icon">
    <link rel="preconnect" href="https://s4.zstatic.net" />
<script src="https://s4.zstatic.net/ajax/libs/vue/3.3.7/vue.global.prod.min.js"></script>
<link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/font-awesome/6.4.2/css/all.min.css" />
<link rel="preconnect" href="https://fonts.googleapis.cn" />
<link rel="preconnect" href="https://fonts.gstatic.cn" crossorigin />
<link
    rel="stylesheet"
    href="https://fonts.googleapis.cn/css2?family=Fira+Code:wght@400;500;600;700&family=Lexend:wght@400;500;600;700;800;900&family=Noto+Sans+SC:wght@400;500;600;700;800;900&display=swap"
/>
<script> const mixins = {}; </script>

<script src="https://polyfill.alicdn.com/v3/polyfill.min.js?features=default"></script>


<script src="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script src="https://s4.zstatic.net/ajax/libs/highlightjs-line-numbers.js/2.8.0/highlightjs-line-numbers.min.js"></script>
<link
    rel="stylesheet"
    href="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/styles/github.min.css"
/>
<script src="/blog/js/lib/highlight.js"></script>


<script src="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/katex.min.js"></script>
<script src="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/contrib/auto-render.min.js"></script>
<link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/katex.min.css" />
<script src="/blog/js/lib/math.js"></script>


<script src="/blog/js/lib/preview.js"></script>









<link rel="stylesheet" href="/blog/css/main.css" />

<meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/blog/atom.xml" title="YokumiのBlog" type="application/atom+xml">
</head>
<body>
    <div id="layout">
        <transition name="fade">
            <div id="loading" v-show="loading">
                <div id="loading-circle">
                    <h2>LOADING</h2>
                    <p>加载过慢请开启缓存 浏览器默认开启</p>
                    <img src="/blog/images/loading.gif" />
                </div>
            </div>
        </transition>
        <div id="menu" :class="{ hidden: hiddenMenu, 'menu-color': menuColor}">
    <nav id="desktop-menu">
        <a class="title" href="/blog/">
            <span>YOKUMIのBLOG</span>
        </a>
        
        <a href="/blog/">
            <i class="fa-solid fa-house fa-fw"></i>
            <span>&ensp;Home</span>
        </a>
        
        <a href="/blog/about">
            <i class="fa-solid fa-id-card fa-fw"></i>
            <span>&ensp;About</span>
        </a>
        
        <a href="/blog/archives">
            <i class="fa-solid fa-box-archive fa-fw"></i>
            <span>&ensp;Archives</span>
        </a>
        
        <a href="/blog/categories">
            <i class="fa-solid fa-bookmark fa-fw"></i>
            <span>&ensp;Categories</span>
        </a>
        
        <a href="/blog/tags">
            <i class="fa-solid fa-tags fa-fw"></i>
            <span>&ensp;Tags</span>
        </a>
        
    </nav>
    <nav id="mobile-menu">
        <div class="title" @click="showMenuItems = !showMenuItems">
            <i class="fa-solid fa-bars fa-fw"></i>
            <span>&emsp;YOKUMIのBLOG</span>
        </div>
        <transition name="slide">
            <div class="items" v-show="showMenuItems">
                
                <a href="/blog/">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-house fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Home</div>
                    </div>
                </a>
                
                <a href="/blog/about">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-id-card fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">About</div>
                    </div>
                </a>
                
                <a href="/blog/archives">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-box-archive fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Archives</div>
                    </div>
                </a>
                
                <a href="/blog/categories">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-bookmark fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Categories</div>
                    </div>
                </a>
                
                <a href="/blog/tags">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-tags fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Tags</div>
                    </div>
                </a>
                
            </div>
        </transition>
    </nav>
</div>
<transition name="fade">
    <div id="menu-curtain" @click="showMenuItems = !showMenuItems" v-show="showMenuItems"></div>
</transition>

        <div id="main" :class="loading ? 'into-enter-from': 'into-enter-active'">
            <div class="article">
    <div>
        <h1>一些有关Large Language Model(LLM)的学习记录</h1>
    </div>
    <div class="info">
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/2/27
        </span>
        
        <span class="category">
            <a href="/blog/categories/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                科研学习
            </a>
        </span>
        
        
        <span class="tags">
            <span class="icon">
                <i class="fa-solid fa-tags fa-fw"></i>
            </span>
            
            
            <span class="tag">
                
                <a href="/blog/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" style="color: #ff7d73">
                    人工智能
                </a>
            </span>
            
            <span class="tag">
                
                <a href="/blog/tags/LLM/" style="color: #00a596">
                    LLM
                </a>
            </span>
            
            <span class="tag">
                
                <a href="/blog/tags/Python/" style="color: #03a9f4">
                    Python
                </a>
            </span>
            
        </span>
        
    </div>
    
    <div class="content" v-pre>
        <p><strong>前言</strong>：</p>
<blockquote>
<p>由于本人之前对于大模型的学习学的急功近利，偏向于应用（<del>乱用bushi</del>），回头看来对LLM本身过于肤浅，所以浅开一个坑，用于补齐原理性的知识；</p>
<p><del>BUPT课设太TM多了</del>。不定期随便填坑。</p>
</blockquote>
<p><strong>参考资料</strong>：</p>
<blockquote>
<p>【1】［美] 塞巴斯蒂安·拉施卡 著，叶文滔 译. 大模型技术30讲[M].
北京：人民邮电出版社，2024.<br>
【2】Raschka S. <em>Build a Large Language Model (From Scratch)</em>[M].
Shelter Island, NY: Manning, 2024.</p>
</blockquote>
<h1 id="一understanding-llm">一、Understanding LLM</h1>
<h2 id="what-is-llm">1.1 What is LLM?</h2>
<p><strong>定义 1.1</strong>：<strong>Large Language Model</strong></p>
<blockquote>
<p>An LLM is a neural network designed to understand, generate, and
respond to human-like text;</p>
</blockquote>
<ul>
<li>"Large" means, datasets on which it's trained and parameters(the
model's size);
<ul>
<li><strong>parameters</strong> are the adjustable weights in the
network that are optimized during training to predict the next word in a
sequence;</li>
</ul></li>
<li>LLM‘s architecture -- <strong>Transformer</strong>, which enables
them to pay selective attention to different parts of the input when
making predictions;</li>
<li>LLM is also called <strong>Generative AI</strong>(生成式AI);</li>
</ul>
<p>至于其中使用到的两种手段 —— machine learning and deep
learning，将在之后具体阐释。（后者是前者的一个子集，主要区别是是否需要手动提取特征。然而机器学习的经典算法已经快被淘汰了，令人感慨）；</p>
<h2 id="application-of-llm">1.2 Application of LLM</h2>
<p>无需多言，用过都说好；</p>
<h2 id="stages-of-building-and-using-llms">1.3 Stages of building and
using LLMs</h2>
<ul>
<li><strong>Pretraining
预训练</strong>：在多领域的庞大数据集下训练；</li>
<li><strong>Finetuning
微调</strong>：针对特定任务通过特定领域的标注数据集在Pretrained
LLM的基础上进行训练；
<ul>
<li><strong>Instruction Finetuning
指令微调</strong>：简单来说，就是<strong><em>标注数据 = 指令 +
正确答案</em></strong>；</li>
<li><strong>Classification Finetuning
分类微调</strong>：<strong><em>标注数据 = 文本 +
分类标签</em></strong>；</li>
</ul></li>
</ul>
<p>两者的具体区别如下图所示：<img src="https://raw.githubusercontent.com/Yokumii/MyPicBucket/img/img/202504171352752.png"></p>
<h2 id="introducing-the-transformer-architecture">1.4 Introducing the
transformer architecture</h2>
<p>Transformer 的两个重要组件是 <strong>Encoder 编码器</strong> 和
<strong>Decoder
解码器</strong>，编码器将输入文本编码为向量，而解码器解码向量并生成相应文本。区别于传统的全连接型或卷积型，编码器和解码器之间采用
<strong>Self-attention mechanism 自注意力机制</strong> 连接；</p>
<p>在
Transformer模型的基础上，又衍生出了两种不同架构：<strong>Bert(Bidirectional
encoder representations from transformers)</strong> 和
<strong>GPT(Generative pretrained transformers)</strong>；</p>
<p>从全拼也可以看出，这两种模型分别用于不同的任务，Bert模型主要用于
predict masked or hidden words
预测掩码值，适用于文本分类任务，以下是它和 GPT-Model 区别的图示：<img src="https://raw.githubusercontent.com/Yokumii/MyPicBucket/img/img/202504171450135.png"></p>
<p>GPT-Model 又分为：<br>
* <strong>Zero-shot
零样本学习</strong>：能够处理从未在训练数据中见过的任务或类别，即模型在面对新任务时不需要额外的训练和微调也能做出合理的决策；<br>
* <strong>Few-shot
少样本学习</strong>：模型在提供极少量样本或示例时，就能够理解并执行特定任务的能力；泛化能力强；</p>
<p>下图具体说明了两者的区别：<br>
<img src="https://raw.githubusercontent.com/Yokumii/MyPicBucket/img/img/202504171510313.png"></p>
<p>GPT-Model采用 <strong>自监督(Self-supervised learning)</strong>
的方式进行学习，即自标注形式(Self-labeling)。我们可以将句子或者文档中的下一个单词作为模型应该预测的标签进行训练。</p>
<blockquote>
<p><strong>注</strong>：模型能够执行未明确训练的任务的能力称为
<strong>Emergent Ability
涌现</strong>，模型并未针对专门任务进行训练，但是在大量数据集的训练下突然表现出未预料到的新行为或新能力。增加参数量往往会促进涌现行为的出现。</p>
</blockquote>
<h2 id="building-a-large-language-model">1.5 Building a large language
model</h2>
<p><img src="https://raw.githubusercontent.com/Yokumii/MyPicBucket/img/img/202504172235133.png"></p>
<h1 id="零">零、</h1>

    </div>
    
    
    
    
    
    
    
</div>

            <footer id="footer">
    <div id="footer-wrap">
        <div>
            &copy;
            2023 - 2025 YokumiのBlog
            <span id="footer-icon">
                <i class="fa-solid fa-font-awesome fa-fw"></i>
            </span>
            &commat;Yokumi
        </div>
        
    </div>
</footer>

<script>
  var event = new Event('hexo-blog-decrypt');
  window.dispatchEvent(event);
</script>

        </div>
        
        <transition name="fade">
            <div id="preview" ref="preview" v-show="previewShow">
                <img id="preview-content" ref="previewContent" />
            </div>
        </transition>
        
    </div>
    <script src="/blog/js/main.js"></script>
    
    




    
</body>
</html>
